{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa9c166",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050844c",
   "metadata": {},
   "source": [
    "\n",
    "# Workshop on Machine Learning in -tagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c65c6c",
   "metadata": {},
   "source": [
    "Outline for Today\n",
    "1. Navigating the zoo of deep learning libraries â€” why Keras?\n",
    "2. The right level of abstraction, FP\n",
    "3. Constructing models with the functional API\n",
    "4. Useful guidance for setting up an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6484f865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 1us/step\n",
      "65536/57026 [==================================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets.boston_housing import load_data\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a3094b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3bf4678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243cf21",
   "metadata": {},
   "source": [
    "# Building a DNN in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4825ced",
   "metadata": {},
   "source": [
    "A supervised neural network, at it's core, consists of three components.\n",
    "\n",
    "A computation graph, i.e., the network itself with all of it's layers\n",
    "A loss function to penalize a problem dependent notion of incorrectness\n",
    "mean squared error, mean absolute error, huber\n",
    "cross entropy, KL-divergence\n",
    "An optimizer, or a way to learn the parameters of your model with respect to a loss\n",
    "Adam, Adagrad, RMSProp, Adadelta, Vanilla SGD, etc.\n",
    "Ability to anneal\n",
    "Let's start from the canonical version of a DNN, a simple multilayer perceptron-like architecture. Building this structure, i.e., a linear sequence of non-linear transforms, is very easy in Keras.\n",
    "\n",
    "Let's make a network that reads in the 13 inputs from the Boston Housing dataset, has two hidden layers, and one linear output. We'll think about layers as mathematical functions using the Keras functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc2232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e9b79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6f4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the input shape (i.e., how many input features) **without** the batch size\n",
    "x = Input(shape=(13, ))\n",
    "\n",
    "# all Keras Ops look like z = f(z) (like functional programming)\n",
    "h = Dense(20)(x)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "h = Dense(20)(h)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "# our output is a single number, the house price.\n",
    "y = Dense(1)(h)\n",
    "\n",
    "# A model is a conta\n",
    "net = Model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "256f0c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(net, to_file='basic.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1b4613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/sraj/anaconda3/lib/python3.8/site-packages (from pydot) (2.4.7)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c1223",
   "metadata": {},
   "source": [
    "To make this neural net useful, we need to compile it into a DAG (directed, acyclic graph). This is the declarative approach to deep learning. This compilation takes all of your ops, your loss, and your model, and constructs all of the operations to do gradient updates.\n",
    "\n",
    "Let's use the Adam [arXiv/1412.6980] optimizer with a mean squared error loss function, i.e., \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa7d7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c39aad",
   "metadata": {},
   "source": [
    "Now, you're going to want to fit the model to your data. This is made very easy by the .fit member function of a model object. We might want to leave out a subset of our data, say 20%, for validation.\n",
    "\n",
    "In addition, we might want to save a network and impose an ability to stop when a validation loss stops decreasing. Keras introduces this with callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81b30005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecb463d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # if we don't have a decrease of the loss for 10 epochs, terminate training.\n",
    "    EarlyStopping(verbose=True, patience=10, monitor='val_loss'), \n",
    "    # Always make sure that we're saving the model weights with the best val loss.\n",
    "    ModelCheckpoint('model.h5', monitor='val_loss', verbose=True, save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc319dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1548.45154, saving model to model.h5\n",
      "11/11 - 1s - loss: 4590.8374 - val_loss: 1548.4515 - 995ms/epoch - 90ms/step\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: val_loss improved from 1548.45154 to 359.23688, saving model to model.h5\n",
      "11/11 - 0s - loss: 1092.5748 - val_loss: 359.2369 - 36ms/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: val_loss improved from 359.23688 to 211.67174, saving model to model.h5\n",
      "11/11 - 0s - loss: 399.2033 - val_loss: 211.6717 - 37ms/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: val_loss improved from 211.67174 to 123.93632, saving model to model.h5\n",
      "11/11 - 0s - loss: 230.1649 - val_loss: 123.9363 - 47ms/epoch - 4ms/step\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: val_loss improved from 123.93632 to 103.66457, saving model to model.h5\n",
      "11/11 - 0s - loss: 164.7570 - val_loss: 103.6646 - 44ms/epoch - 4ms/step\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: val_loss improved from 103.66457 to 90.10807, saving model to model.h5\n",
      "11/11 - 0s - loss: 130.5685 - val_loss: 90.1081 - 40ms/epoch - 4ms/step\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: val_loss improved from 90.10807 to 80.60356, saving model to model.h5\n",
      "11/11 - 0s - loss: 109.7250 - val_loss: 80.6036 - 62ms/epoch - 6ms/step\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: val_loss improved from 80.60356 to 76.07992, saving model to model.h5\n",
      "11/11 - 0s - loss: 93.4425 - val_loss: 76.0799 - 54ms/epoch - 5ms/step\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: val_loss improved from 76.07992 to 72.61452, saving model to model.h5\n",
      "11/11 - 0s - loss: 84.9889 - val_loss: 72.6145 - 43ms/epoch - 4ms/step\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: val_loss improved from 72.61452 to 70.86739, saving model to model.h5\n",
      "11/11 - 0s - loss: 77.5179 - val_loss: 70.8674 - 45ms/epoch - 4ms/step\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: val_loss improved from 70.86739 to 69.30410, saving model to model.h5\n",
      "11/11 - 0s - loss: 73.3383 - val_loss: 69.3041 - 39ms/epoch - 4ms/step\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: val_loss improved from 69.30410 to 68.76396, saving model to model.h5\n",
      "11/11 - 0s - loss: 70.8478 - val_loss: 68.7640 - 44ms/epoch - 4ms/step\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 68.76396\n",
      "11/11 - 0s - loss: 68.1676 - val_loss: 68.8731 - 31ms/epoch - 3ms/step\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: val_loss improved from 68.76396 to 66.81857, saving model to model.h5\n",
      "11/11 - 0s - loss: 66.3988 - val_loss: 66.8186 - 44ms/epoch - 4ms/step\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: val_loss improved from 66.81857 to 66.52633, saving model to model.h5\n",
      "11/11 - 0s - loss: 65.0258 - val_loss: 66.5263 - 59ms/epoch - 5ms/step\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: val_loss improved from 66.52633 to 66.36082, saving model to model.h5\n",
      "11/11 - 0s - loss: 63.7727 - val_loss: 66.3608 - 49ms/epoch - 4ms/step\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 66.36082\n",
      "11/11 - 0s - loss: 63.0115 - val_loss: 66.7295 - 35ms/epoch - 3ms/step\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: val_loss improved from 66.36082 to 64.96206, saving model to model.h5\n",
      "11/11 - 0s - loss: 62.8032 - val_loss: 64.9621 - 39ms/epoch - 4ms/step\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: val_loss improved from 64.96206 to 64.74641, saving model to model.h5\n",
      "11/11 - 0s - loss: 61.6876 - val_loss: 64.7464 - 43ms/epoch - 4ms/step\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: val_loss improved from 64.74641 to 63.82230, saving model to model.h5\n",
      "11/11 - 0s - loss: 59.8620 - val_loss: 63.8223 - 46ms/epoch - 4ms/step\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: val_loss improved from 63.82230 to 62.95761, saving model to model.h5\n",
      "11/11 - 0s - loss: 58.8999 - val_loss: 62.9576 - 47ms/epoch - 4ms/step\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 62.95761\n",
      "11/11 - 0s - loss: 58.6808 - val_loss: 63.1393 - 30ms/epoch - 3ms/step\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: val_loss improved from 62.95761 to 62.27544, saving model to model.h5\n",
      "11/11 - 0s - loss: 58.7134 - val_loss: 62.2754 - 43ms/epoch - 4ms/step\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: val_loss improved from 62.27544 to 61.76701, saving model to model.h5\n",
      "11/11 - 0s - loss: 57.4537 - val_loss: 61.7670 - 57ms/epoch - 5ms/step\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: val_loss improved from 61.76701 to 61.13292, saving model to model.h5\n",
      "11/11 - 0s - loss: 59.2299 - val_loss: 61.1329 - 41ms/epoch - 4ms/step\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 61.13292\n",
      "11/11 - 0s - loss: 56.4764 - val_loss: 61.8929 - 28ms/epoch - 3ms/step\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: val_loss improved from 61.13292 to 60.63449, saving model to model.h5\n",
      "11/11 - 0s - loss: 55.5908 - val_loss: 60.6345 - 41ms/epoch - 4ms/step\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 60.63449\n",
      "11/11 - 0s - loss: 55.0544 - val_loss: 62.0014 - 23ms/epoch - 2ms/step\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: val_loss improved from 60.63449 to 60.42791, saving model to model.h5\n",
      "11/11 - 0s - loss: 55.4841 - val_loss: 60.4279 - 42ms/epoch - 4ms/step\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: val_loss improved from 60.42791 to 59.47559, saving model to model.h5\n",
      "11/11 - 0s - loss: 53.9079 - val_loss: 59.4756 - 46ms/epoch - 4ms/step\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 59.47559\n",
      "11/11 - 0s - loss: 53.5104 - val_loss: 60.8782 - 23ms/epoch - 2ms/step\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 59.47559\n",
      "11/11 - 0s - loss: 55.0334 - val_loss: 59.5069 - 24ms/epoch - 2ms/step\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 59.47559\n",
      "11/11 - 0s - loss: 53.1863 - val_loss: 59.8671 - 30ms/epoch - 3ms/step\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 59.47559\n",
      "11/11 - 0s - loss: 52.3164 - val_loss: 59.8940 - 28ms/epoch - 3ms/step\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: val_loss improved from 59.47559 to 59.46112, saving model to model.h5\n",
      "11/11 - 0s - loss: 52.2319 - val_loss: 59.4611 - 44ms/epoch - 4ms/step\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: val_loss improved from 59.46112 to 59.40273, saving model to model.h5\n",
      "11/11 - 0s - loss: 51.9329 - val_loss: 59.4027 - 38ms/epoch - 3ms/step\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: val_loss improved from 59.40273 to 58.89488, saving model to model.h5\n",
      "11/11 - 0s - loss: 51.4858 - val_loss: 58.8949 - 47ms/epoch - 4ms/step\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 58.89488\n",
      "11/11 - 0s - loss: 51.0415 - val_loss: 59.1214 - 33ms/epoch - 3ms/step\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: val_loss improved from 58.89488 to 57.70507, saving model to model.h5\n",
      "11/11 - 0s - loss: 51.3653 - val_loss: 57.7051 - 47ms/epoch - 4ms/step\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 57.70507\n",
      "11/11 - 0s - loss: 50.3275 - val_loss: 58.9981 - 30ms/epoch - 3ms/step\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 57.70507\n",
      "11/11 - 0s - loss: 50.1148 - val_loss: 57.9010 - 25ms/epoch - 2ms/step\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 57.70507\n",
      "11/11 - 0s - loss: 49.8872 - val_loss: 57.9046 - 26ms/epoch - 2ms/step\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: val_loss improved from 57.70507 to 57.32689, saving model to model.h5\n",
      "11/11 - 0s - loss: 49.6910 - val_loss: 57.3269 - 41ms/epoch - 4ms/step\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: val_loss improved from 57.32689 to 57.11490, saving model to model.h5\n",
      "11/11 - 0s - loss: 49.5289 - val_loss: 57.1149 - 39ms/epoch - 4ms/step\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 57.11490\n",
      "11/11 - 0s - loss: 49.9221 - val_loss: 57.1679 - 23ms/epoch - 2ms/step\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: val_loss improved from 57.11490 to 55.79229, saving model to model.h5\n",
      "11/11 - 0s - loss: 49.1517 - val_loss: 55.7923 - 45ms/epoch - 4ms/step\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: val_loss improved from 55.79229 to 55.61658, saving model to model.h5\n",
      "11/11 - 0s - loss: 48.1890 - val_loss: 55.6166 - 47ms/epoch - 4ms/step\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: val_loss improved from 55.61658 to 55.04955, saving model to model.h5\n",
      "11/11 - 0s - loss: 47.6050 - val_loss: 55.0496 - 47ms/epoch - 4ms/step\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: val_loss improved from 55.04955 to 54.35351, saving model to model.h5\n",
      "11/11 - 0s - loss: 47.8678 - val_loss: 54.3535 - 41ms/epoch - 4ms/step\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: val_loss improved from 54.35351 to 53.84919, saving model to model.h5\n",
      "11/11 - 0s - loss: 48.3721 - val_loss: 53.8492 - 39ms/epoch - 4ms/step\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 53.84919\n",
      "11/11 - 0s - loss: 47.7149 - val_loss: 54.8667 - 34ms/epoch - 3ms/step\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: val_loss improved from 53.84919 to 52.77029, saving model to model.h5\n",
      "11/11 - 0s - loss: 47.7470 - val_loss: 52.7703 - 45ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 52.77029\n",
      "11/11 - 0s - loss: 46.3195 - val_loss: 55.1034 - 25ms/epoch - 2ms/step\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: val_loss improved from 52.77029 to 52.31355, saving model to model.h5\n",
      "11/11 - 0s - loss: 47.3219 - val_loss: 52.3135 - 43ms/epoch - 4ms/step\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: val_loss improved from 52.31355 to 52.03003, saving model to model.h5\n",
      "11/11 - 0s - loss: 47.1590 - val_loss: 52.0300 - 43ms/epoch - 4ms/step\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 52.03003\n",
      "11/11 - 0s - loss: 50.1302 - val_loss: 53.7437 - 54ms/epoch - 5ms/step\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: val_loss improved from 52.03003 to 51.56026, saving model to model.h5\n",
      "11/11 - 0s - loss: 47.3402 - val_loss: 51.5603 - 46ms/epoch - 4ms/step\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 51.56026\n",
      "11/11 - 0s - loss: 45.6795 - val_loss: 52.1486 - 30ms/epoch - 3ms/step\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: val_loss improved from 51.56026 to 50.31504, saving model to model.h5\n",
      "11/11 - 0s - loss: 45.2182 - val_loss: 50.3150 - 38ms/epoch - 3ms/step\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: val_loss improved from 50.31504 to 50.12479, saving model to model.h5\n",
      "11/11 - 0s - loss: 45.7490 - val_loss: 50.1248 - 40ms/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "history = net.fit(X_train, y_train, validation_split=0.2, epochs=60, verbose=2, callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58dd2112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbc6bfa3af0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0681fcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 53.1127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53.11274337768555"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now I can evaluate what my \n",
    "net.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661dae81",
   "metadata": {},
   "source": [
    "Though not a formal test, let's see how the output distribution looks between the true y_test and the \n",
    " from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7f517df",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = net.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fdf1260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbc6c23cc40>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, bins, _ = plt.hist(yhat, histtype='step', label=r'$\\hat{y}$')\n",
    "plt.hist(y_test, bins=bins, histtype='step', label=r'$y_{\\mathsf{true}}$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f13d89",
   "metadata": {},
   "source": [
    "So far, we've done nothing that you can't do in TMVA (maybe besides the Adam optimizer), so why use Keras? You gain the ability to work with things like sequence data, images, and although maybe not necessary for the Boston dataset, you are able to compose arbitrary graphs!\n",
    "\n",
    "Let's quickly show how we can create a residual model with dropout for our boston housing model.\n",
    "\n",
    "The basis of a residual block is the idea that learning the residual components beyond the identity function decouples some of the difficulties in learning complex funcionals. The idea is instead of having , we want  where our  is now tasked with learning a residual on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91cec484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "762275b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the input shape (i.e., how many input features) **without** the batch size\n",
    "x = Input(shape=(13, ))\n",
    "\n",
    "# we want this layer to be normal, but skip into a layer downstream\n",
    "skip = Dense(20)(x)\n",
    "h = Dropout(0.5)(skip)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "h = Dense(20)(h)\n",
    "skip = add([h, skip])\n",
    "h = Dropout(0.5)(skip)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "h = Dense(20)(h)\n",
    "h = add([h, skip])\n",
    "h = Dropout(0.5)(h)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "# our output is a single number, the house price.\n",
    "y = Dense(1)(h)\n",
    "\n",
    "# A model is a conta\n",
    "resnet = Model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9486c553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(resnet, to_file='arch.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593e12d",
   "metadata": {},
   "source": [
    "# Dealing with Sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540c19be",
   "metadata": {},
   "source": [
    "Most applications of sequence / recurrent models around grounded in natural language processing or stock price analysis. Let's use a hypothetical example that is grounded in Physics.\n",
    "\n",
    "To ground this problem, let's say we're building a network for an analysis that reads in all jets and all photons from two different streams. There are arbitrary numbers of jets and photons, so we need a model that can handle a sequence.\n",
    "\n",
    "Let's have the following two constraints on our jets and photons:\n",
    "\n",
    "We have a maximum of 8 jets with 6 features\n",
    "We have a maximum of 2 photons with 11 features\n",
    "We can order our physics object by some value, say , and construct a recurrent neural network, in particular a Bidirectional LSTM, to learn a function that maps this to signal/background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0fe61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, concatenate, Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd402c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you wound need to structure your input into this format such that it has shape (nb_samples, nb_objects, nb_features)\n",
    "n_jets = 8\n",
    "n_jet_feats = 6\n",
    "\n",
    "n_photons =  2\n",
    "n_photon_feats = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d1e8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "jets = Input(shape=(n_jets, n_jet_feats), name='jets')\n",
    "photons = Input(shape=(n_photons, n_photon_feats), name='photons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d169825",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = concatenate([\n",
    "    Bidirectional(LSTM(10, name='jet_lstm'))(jets), \n",
    "    Bidirectional(LSTM(10, name='photon_lstm'))(photons), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0b1eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Activation('sigmoid', name='sigmoid')(Dense(1, name='logistic')(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23d9b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Model([jets, photons], y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8409225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(rnn, to_file='arnn.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5059a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
